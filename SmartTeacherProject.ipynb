{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5+pW6Id4e+yLNoEOUpNat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zark-byte/Embedding-Project/blob/main/SmartTeacherProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGJSuO9kSVBd",
        "outputId": "50f3ee15-b482-44ee-cdae-5f087192b71f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 简单文本生成示例 ===\n",
            "Artificial intelligence can help you understand the details of what you are doing and how you can use it to improve your performance.\n",
            "\n",
            "Automated AI will help you understand how you are doing and how you can use it to improve your performance.\n",
            "\n",
            "It's all about your performance, not your performance.\n",
            "\n",
            "As a programmer, I like to think of the performance of a program as being about the amount of code that is executed. In some situations, however, this is not the case, as the same code will be executed, but more code will be executed. One way to visualize the performance of a program is to use a graph. In a graph, you can see that the CPU is executing on the graph, and the memory is being used to store the data in the graph. The CPU will be executing more code on that graph than it does on the memory.\n",
            "\n",
            "In this case, there are some data structures that exist within the graph. These structures are called memory pools. These memory pools are called memory allocators. In other words, the memory allocators are allocated to the memory pool of the graph.\n",
            "\n",
            "And in order to understand the memory pool, you should understand the memory allocator as well. Memory allocators are a special kind of memory that\n"
          ]
        }
      ],
      "source": [
        "# 安装必要的库\n",
        "!pip install transformers torch\n",
        "\n",
        "# 导入库\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "# 加载文本生成 pipeline\n",
        "# 我们使用一个较小的模型以便在Colab中快速运行\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)  # 设置随机种子，保证结果可复现\n",
        "\n",
        "# 简单文本生成示例\n",
        "print(\"=== 简单文本生成示例 ===\")\n",
        "prompt = \"Artificial intelligence can\"\n",
        "results = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(results[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 互动文本生成\n",
        "print(\"\\n=== 互动文本生成 ===\")\n",
        "def interactive_advanced_generator():\n",
        "    while True:\n",
        "        prompt = input(\"\\n请输入一个文本开头（输入'quit'退出）: \")\n",
        "        if prompt.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        length = input(\"请输入生成的最大长度（默认100）: \")\n",
        "        length = int(length) if length.isdigit() else 100\n",
        "\n",
        "        num_samples = input(\"请输入生成的样本数（1-3，默认1）: \")\n",
        "        num_samples = int(num_samples) if num_samples.isdigit() and 1 <= int(num_samples) <=3 else 1\n",
        "\n",
        "        print(\"\\n生成中...\")\n",
        "        results = generator(\n",
        "            prompt,\n",
        "            max_length=length,\n",
        "            num_return_sequences=num_samples,\n",
        "            temperature=0.7,  # 控制随机性，值越小越确定\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"\\n样本 {i+1}:\")\n",
        "            print(result['generated_text'])\n",
        "\n",
        "# 运行互动生成器\n",
        "interactive_advanced_generator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTWk1-fHS-Jf",
        "outputId": "786986f0-18a6-4e43-983e-bcb13b82bd15"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 互动文本生成 ===\n",
            "\n",
            "请输入一个文本开头（输入'quit'退出）: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "def interactive_qa():\n",
        "  context = \"\"\"\n",
        "  语言模型（Language Model, LM）是自然语言处理领域的核心模型之一，其基本任务是预测一个词序列中下一个词出现的概率。简单来说，语言模型旨在回答这样一个问题：“在给定前文的情况下，下一个最可能出现的词是什么？”例如，当输入“今天天气很”时，模型可能预测“好”“热”或“糟糕”等词，从而生成合乎语言习惯的完整表达。\n",
        "\n",
        "从技术路径来看，语言模型的发展经历了从统计方法到神经网络模型的演进。早期语言模型主要基于 n-gram 统计方法，通过计算词语共现频率来估计概率。虽然结构简单、计算高效，但受限于“维度灾难”和长距离依赖捕捉能力弱的问题。随着深度学习的发展，以 RNN、LNN、Transformer 为代表的神经网络语言模型逐渐成为主流。特别是基于 Transformer 架构的模型，借助自注意力机制能够有效捕捉长距离语义依赖，显著提升了语言建模的性能。\n",
        "\n",
        "近年来，以 GPT、BERT、T5 等为代表的大型语言模型（Large Language Models, LLMs）通过海量语料训练和巨大参数量构建，展现出强大的泛化能力和上下文理解能力。它们不仅能够高质量完成文本生成、自动补全、机器翻译等传统任务，还可实现问答、摘要、代码生成乃至多模态理解等复杂任务，逐步趋近通用人工智能的初级形态。\n",
        "\n",
        "语言模型的核心训练目标是学习自然语言的潜在概率分布，从而在语义连贯性、语法正确性和语境适配性之间取得平衡。随着模型规模的不断扩大和训练方法的持续优化，语言模型正成为推动人工智能语言理解与生成能力进步的关键力量。\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  print (\"\\n已知信息：\")\n",
        "  print(context)\n",
        "  while True:\n",
        "    question = input(\"\\n请输入一个问题（输入'quit'退出）：\")\n",
        "    if question.lower() == 'quit':\n",
        "      break\n",
        "    result = question_answerer(question=question, context=context)\n",
        "    print(f\"答案：{result['answer']}\")\n",
        "    print(f\"置信度：{result['score']:.4f}\")\n",
        "\n",
        "interactive_qa()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "EY0WqBW7TQe1",
        "outputId": "5c52e048-cf9a-42fd-d181-104d89cfb871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "已知信息：\n",
            "\n",
            "  语言模型（Language Model, LM）是自然语言处理领域的核心模型之一，其基本任务是预测一个词序列中下一个词出现的概率。简单来说，语言模型旨在回答这样一个问题：“在给定前文的情况下，下一个最可能出现的词是什么？”例如，当输入“今天天气很”时，模型可能预测“好”“热”或“糟糕”等词，从而生成合乎语言习惯的完整表达。\n",
            "\n",
            "从技术路径来看，语言模型的发展经历了从统计方法到神经网络模型的演进。早期语言模型主要基于 n-gram 统计方法，通过计算词语共现频率来估计概率。虽然结构简单、计算高效，但受限于“维度灾难”和长距离依赖捕捉能力弱的问题。随着深度学习的发展，以 RNN、LNN、Transformer 为代表的神经网络语言模型逐渐成为主流。特别是基于 Transformer 架构的模型，借助自注意力机制能够有效捕捉长距离语义依赖，显著提升了语言建模的性能。\n",
            "\n",
            "近年来，以 GPT、BERT、T5 等为代表的大型语言模型（Large Language Models, LLMs）通过海量语料训练和巨大参数量构建，展现出强大的泛化能力和上下文理解能力。它们不仅能够高质量完成文本生成、自动补全、机器翻译等传统任务，还可实现问答、摘要、代码生成乃至多模态理解等复杂任务，逐步趋近通用人工智能的初级形态。\n",
            "\n",
            "语言模型的核心训练目标是学习自然语言的潜在概率分布，从而在语义连贯性、语法正确性和语境适配性之间取得平衡。随着模型规模的不断扩大和训练方法的持续优化，语言模型正成为推动人工智能语言理解与生成能力进步的关键力量。\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "请输入一个问题（输入'quit'退出）：你是谁\n",
            "答案：BERT\n",
            "置信度：0.0010\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2043182446.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"置信度：{result['score']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0minteractive_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2043182446.py\u001b[0m in \u001b[0;36minteractive_qa\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n请输入一个问题（输入'quit'退出）：\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}