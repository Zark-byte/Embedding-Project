import jieba
from gensim.models import Word2Vec
import re
import random

# 1. 准备语料并训练模型（使用扩展语料提高游戏体验）
corpus = [
  """好的，我们来将这些核心句子进行深度扩写，构建一篇关于自然语言处理、词向量和机器学习的详尽文章。扩写将遵循逻辑递进的结构，从宏观领域介绍到具体技术细节，再探讨应用与未来展望，力求内容翔实、深入浅出。

从符号到语义：自然语言处理、词向量与机器学习的深度探索

第一章：人工智能的瑰宝——自然语言处理

“自然语言处理是人工智能的重要分支”——这句话看似简单，却承载着人工智能领域最宏大、最复杂的梦想之一。自然语言处理，简称NLP，其终极目标是打破人与机器之间的沟通壁垒，让计算机能够像人类一样理解、阐释、甚至生成自然语言。

要深入理解NLP的重要性，我们必须将其置于人工智能发展的宏大图景中。人工智能的核心是让机器具备智能，能够执行通常需要人类智慧的任务。这些任务涵盖了感知（如计算机视觉、语音识别）、推理、知识表达和决策等多个层面。而自然语言，作为人类知识、文化、思想和情感最主要的载体，无疑是人工智能必须攻克的“终极高地”。如果机器无法理解语言，那么它对于人类世界的认知将是极其片面和肤浅的。因此，NLP不仅是AI的一个分支，更是连接AI与人类社会的桥梁，是衡量人工智能发展水平的关键标尺。

NLP的研究范围极其广泛，从上世纪50年代的早期机器翻译研究（如著名的乔治敦-IBM实验）到今天无处不在的智能助手、机器翻译引擎和搜索引擎，其发展历程波澜壮阔。早期的NLP系统主要依赖于基于规则的方法。语言学家和计算机科学家们试图将人类语言的语法规则（如句法结构、词性标注）和词典知识硬编码到计算机中。例如，要处理“The cat sat on the mat”这个句子，系统需要预先定义“cat”是名词、“sat”是动词、“on”是介词，并规定“名词+动词+介词+名词”是一种合法的句子结构。这种方法在受限领域或简单句子上可能有效，但面对人类语言无穷的创造力、复杂的歧义现象（如“I saw the man with the telescope”）、丰富的俚语和新词时，便显得力不从心。规则系统脆弱、难以扩展，且构建成本极高，这成为了NLP发展的第一个主要瓶颈。

直到20世纪末21世纪初，随着互联网的普及和计算能力的提升，一种新的范式彻底改变了NLP乃至整个AI领域的研究路径——这就是基于统计的方法，亦即机器学习。NLP的研究重心从“教计算机语法规则”转向了“让计算机从海量数据中自行学习语言规律”。这一转变是革命性的，它意味着我们不再需要穷尽一切可能去手动编写规则，而是可以利用数据驱动的方式，让模型自动发现语言中的概率性模式。例如，通过分析数十亿计的网页文本，计算机会发现“猫”这个词后面出现“狗”的概率，远高于出现“哲学”的概率。这种从大数据中挖掘出的共现统计规律，为语言理解提供了远比规则系统更坚实、更灵活的基础。至此，NLP与机器学习紧密地结合在了一起，并逐步演进为我们今天所见的，以深度学习为主导的现代自然语言处理。

第二章：数据的燃料——机器学习的基石

“机器学习需要大量的数据训练模型”，这揭示了现代AI，尤其是数据驱动范式的核心特征。我们可以将一个机器学习模型，特别是复杂的深度学习模型，比喻为一个极其聪明但缺乏经验的学生。数据，就是这位学生的教科书、习题集和百科全书。没有足够数量和高质量的数据，再优秀的算法也无法学会完成特定任务。

为什么数据如此关键？因为机器学习的本质是通过优化算法，调整模型内部数百万甚至数十亿的参数，使得模型能够从输入数据映射到期望的输出。这个过程被称为“训练”。以图像分类为例，当我们向一个卷积神经网络展示成千上万张标注为“猫”和“狗”的图片时，模型会不断地比较自己的预测（“这张图片是猫”）与真实答案（“这是狗”）之间的差异（即损失函数），然后通过反向传播算法微调其内部参数，以减少下一次预测的误差。经过海量数据的反复“学习”，模型最终能够捕捉到“猫”和“狗”最本质、最细微的视觉特征（如耳朵形状、面部轮廓），从而对从未见过的新图片做出准确判断。

在NLP领域，数据的角色同样至关重要。而且，NLP任务所需的数据往往是非结构化的文本数据，其处理难度更大。这些数据构成了模型的训练集、验证集和测试集。数据的“大量”体现在三个维度：规模、质量和多样性。
1.  规模：深度学习模型具有巨大的容量，足够的数据量是防止模型“过拟合”的关键。过拟合是指模型仅仅记住了训练数据中的噪声和特定样本特征，而非学习到底层的通用规律，导致其在未见数据上表现糟糕。更多的数据有助于模型泛化到更普遍的情况。
2.  质量：垃圾进，垃圾出。如果训练数据充满了错误标注、偏见和噪声，模型学到的也将是错误和有偏见的知识。例如，一个用带有性别偏见的数据训练的简历筛选系统，可能会不公正地偏向男性候选人。
3.  多样性：数据需要覆盖任务可能遇到的各种场景和语言风格。例如，训练一个客服机器人，如果数据只包含正式书面语，那么它在处理口语化、带有错别字或方言的用户提问时就会表现不佳。

正是由于对数据的极度渴求，互联网成为了NLP发展的最大宝藏。维基百科、新闻网站、社交媒体、书籍扫描项目等提供了前所未有的海量文本语料库，为训练强大的语言模型奠定了坚实的基础。这也引出了下一个关键环节：如何将这些原始的、符号化的文本数据，转化为机器可以理解和处理的数值形式？这就需要进行精细的文本预处理。

第三章：文本的基石——预处理与分词

“分词是文本预处理的关键步骤”。在将文本数据“喂”给机器学习模型之前，必须对其进行一系列规范化处理，统称为文本预处理。这个过程的目的在于清洗噪声、标准化格式，并将人类可读的文本转换成结构化的、机器可读的基本单元。预处理通常包括以下步骤：
•   数据清洗：去除HTML标签、无关的符号、异常字符等。

•   文本规范化：将字母统一为小写、处理缩写词（如将“isn‘t”扩展为“is not”）、纠正常见拼写错误等。

•   停用词去除：过滤掉出现频率极高但语义信息较少的词语，如英文中的“the", “a", “is"，中文中的“的”、“是”、“在”等，以降低数据维度和噪声。

•   词干提取与词形还原：将词语的不同形态归并到其原形或词干，如将“running", “ran", “runs”都还原为“run”。

在所有这些步骤中，对于许多语言（尤其是汉语这样的孤立语）而言，分词是最基础、最核心的一步。对于英语等空格分隔的语言，分词相对简单，通常以空格和标点进行切分即可。但对于汉语、日语等语言，词语之间没有天然的分隔符，分词就成了一个必须首先解决的复杂任务。

例如，句子“自然语言处理是人工智能的重要分支”需要被正确地切分为“自然语言处理 / 是 / 人工智能 / 的 / 重要 / 分支”，而不是错误的“自然 / 语言 / 处理 / 是 / 人工 / 智能 / 的 / 重要 / 分支”。错误的分词会直接导致后续语义理解的彻底失败。中文分词技术经历了从基于词典的匹配方法，到基于统计的机器学习方法（如隐马尔可夫模型、条件随机场），再到如今基于深度学习序列标注方法的演进。现代分词工具（如Jieba、HanLP）已经能够达到很高的准确率，但它们依然面临歧义切分（如“乒乓球拍/卖/完了” vs “乒乓球/拍卖/完了”）、新词发现等挑战。

分词之后，我们得到了一系列离散的符号（词语）。但机器学习模型，无论是简单的逻辑回归还是复杂的Transformer，其本质都是进行数学运算。它们无法直接处理“猫”、“狗”这样的符号。因此，我们需要一种方法，将这些符号数值化，即转换为模型能够处理的数字形式。最直观的方法是独热编码。

独热编码为词典中的每个词分配一个唯一的、很长的二进制向量。向量的长度等于词典的大小（比如50000个词）。对于某个特定的词，它的向量表示只有在对应词典索引的位置上是1，其他位置全是0。例如：
•   “猫” = [1, 0, 0, 0, ..., 0]

•   “狗” = [0, 1, 0, 0, ..., 0]

•   “数学” = [0, 0, 1, 0, ..., 0]

这种方法简单明了，但存在两个致命缺陷：1. 维度灾难：词典动辄数万词，向量维度极高，计算和存储效率低下。2. 语义鸿沟：每个词都被表示为完全独立的向量，无法体现词与词之间的任何关系。“猫”和“狗”都是宠物，语义上很接近，但它们的独热向量点积为0，模型无法感知这种相似性。这引出了一个核心问题：我们能否找到一种更好的词语表示方法，既能降低维度，又能捕捉词语的语义信息？

第四章：革命的种子——Word2Vec与词向量的诞生

“Word2Vec可以将词语转换为向量”，而“词向量能够捕捉词语的语义信息”。这正是Word2Vec在2013年由Tomas Mikolov等人在谷歌提出时，为NLP领域带来的革命性突破。它成功地解决了独热编码的弊端，提供了一种将词语映射到低维、稠密实数向量空间的方法。

这些低维向量（通常50维到300维）被称为词嵌入。Word2Vec的核心思想源于分布假说：“一个词的含义可以由它周围的词来定义”。换句话说，出现在相似上下文中的词语，其含义也相近。Word2Vec通过一个浅层神经网络模型来学习这种上下文关系，从而得到词的向量表示。

Word2Vec主要有两种模型架构：CBOW和Skip-gram。
•   连续词袋模型：CBOW模型的训练目标是根据上下文词语来预测中心词。例如，给定句子“The ___ sat on the mat”，模型需要根据上下文“The", “sat", “on", “the", “mat”来预测出中心词“cat”。CBOW在训练时将所有的上下文词向量求平均后输入网络，因此得名“词袋”。它训练速度快，对高频词的表征更好。

•   Skip-gram模型：Skip-gram与CBOW相反，其训练目标是根据中心词来预测其周围的上下文词语。例如，给定中心词“cat”，模型需要预测出它周围一定窗口内的词，如“The", “sat", “on", “the", “mat”。Skip-gram在处理小型数据集和稀有词时表现更好，能提供更精细的向量表示。

训练完成后，模型丢弃掉用于预测的输出层，只保留隐藏层的权重矩阵。这个矩阵的每一行，就对应着词典中一个词的词向量。这些向量的神奇之处在于，它们不仅是一个数字标识符，其在高维空间中的几何关系恰好编码了词语的语义和语法关系。

•   语义相似性：语义相近的词，其向量在空间中的距离（如余弦相似度）会很近。例如，“国王”和“王后”的向量距离，会远小于“国王”和“苹果”的距离。

•   类比关系：词向量空间可以进行经典的向量运算。最著名的例子是：vec(“国王”) - vec(“男人”) + vec(“女人”) ≈ vec(“王后”)。类似的，vec(“北京”) - vec(“中国”) + vec(“法国”) ≈ vec(“巴黎”)。这表明模型已经捕捉到了“首都-国家”这种抽象的语义关系。

词向量的出现，使得语义相似度这一NLP核心任务的实现变得前所未有的直接和有效。“语义相似度是自然语言处理的重要任务”，它在信息检索、智能问答、重复检测、推荐系统等场景中至关重要。在此之前，衡量两个词或两段文本的相似度非常困难，通常依赖于人工构建的同义词词典（如WordNet）或复杂的语义分析。而现在，我们只需要计算两个词向量的余弦相似度，就能得到一个量化的、有说服力的语义相关性分数。这为下游的NLP应用（如情感分析、文本分类、机器翻译）提供了强大的特征表示基础。

第五章：深度的力量——深度学习与神经网络

“深度学习是机器学习的一个子领域”，而“神经网络由多个神经元相互连接组成”。深度学习并非全新的概念，其基础——人工神经网络，早在20世纪40年代就已提出。但直到最近十年，由于大数据、强大算力（特别是GPU）和算法改进（如ReLU激活函数、Dropout、批量归一化）的三重驱动，深度学习才爆发出巨大的潜力，并在图像识别、语音识别和NLP等领域取得了突破性进展。

所谓“深度”，指的是神经网络中隐藏层的数量较多。一个基本的人工神经元，也称作感知机，是对生物神经元的简化模拟。它接收多个输入信号，每个信号乘以一个权重（代表连接强度），然后将加权和通过一个非线性激活函数（如Sigmoid、Tanh、ReLU）产生输出。单个神经元能力有限，只能进行线性划分。

但当成千上万个这样的神经元按照层级结构连接起来时，就形成了神经网络。输入层的神经元接收原始数据（如图像像素、词向量），经过一个或多个隐藏层的逐层变换，最终在输出层得到结果。每一层都可以被视为一个特征提取器。浅层网络可能学习到一些边缘、纹理等低级特征，而更深层的网络则能够组合这些低级特征，形成更复杂、更抽象的高级特征表示。例如，在图像识别中，深层网络可能逐步组合出“眼睛”、“鼻子”、“轮廓”等概念，最终识别出“这是一张人脸”。

在NLP领域，简单的神经网络（如MLP）可以用于文本分类等任务。但处理序列数据（如句子）需要更特殊的结构，因为句子中的词语是有顺序的。这催生了循环神经网络（RNN）及其变体（如LSTM、GRU）。RNN具有“记忆”功能，能够处理前后依赖的序列信息，非常适合语言建模、机器翻译等任务。然而，RNN存在梯度消失/爆炸问题，难以捕捉长距离依赖。

2017年，Transformer模型架构的提出，彻底改变了NLP的格局。Transformer完全基于自注意力机制，可以并行处理序列中的所有词，并动态地计算每个词对于其他所有词的注意力权重，从而高效地捕捉长距离依赖关系。以Transformer为核心构建的大型预训练语言模型，如BERT、GPT系列，成为了当今NLP的主流。这些模型首先在超大规模语料库上进行无监督预训练，学习通用的语言表示，然后只需少量的标注数据针对特定任务进行微调，就能取得极佳的性能。可以说，Word2Vec开启了静态词向量的时代，而BERT等模型则将上下文信息动态地融入每个词的表示中，使得“一词多义”得到了很好的解决。

第六章：应用、挑战与未来展望

词向量和深度学习技术已经深刻地融入了我们的日常生活。搜索引擎通过理解查询词的语义，返回更相关的结果；智能助手（如Siri、Alexa）能够进行流畅的对话；机器翻译（如Google Translate）的质量达到了前所未有的高度；社交媒体能够自动过滤有害信息；电子邮件系统可以自动分类和撰写回复。

然而，NLP领域依然面临着严峻的挑战。
•   偏见与公平性：模型从互联网数据中学到的人类社会偏见（性别、种族、文化等）可能会被放大和固化，导致不公平的决策。

•   可解释性：深度神经网络如同“黑箱”，我们难以理解其内部具体的决策过程，这在医疗、司法等高风险领域是一个重大隐患。

•   常识推理：模型虽然能处理海量文本知识，但缺乏人类与生俱来的物理常识和社会常识，容易犯下低级的逻辑错误。

•   数据依赖与能耗：训练大型模型需要巨大的计算资源和能源消耗，引发了关于其环境成本和经济可行性的思考。

展望未来，NLP的研究正朝着几个方向演进：1. 更大规模与多模态：探索万亿参数级别的模型，并融合文本、图像、声音等多模态信息进行学习。2. 提高效率与降低能耗：研究模型压缩、知识蒸馏、更高效的架构，让强大的模型能在资源受限的设备上运行。3. 增强可解释性与可控性：让模型的决策过程更透明、更符合人类的价值观和意图。4. 探索新的学习范式：如小样本学习、自监督学习，减少对大量标注数据的依赖。

结论

回顾整篇文章，我们从“自然语言处理是人工智能的重要分支”这一宏观命题出发，逐步深入到“分词”、“Word2Vec”、“词向量”、“神经网络”等微观技术细节。这条路径清晰地展示了现代NLP的发展逻辑：以数据为燃料，以机器学习为引擎，通过词向量等技术将符号世界映射到向量空间，再利用深度学习模型强大的函数拟合能力，从数据中自动学习复杂的语言规律。Word2Vec作为其中承上启下的关键一环，不仅本身是一项高效实用的技术，其“由词嵌入获取语义”的核心思想，更是深远地影响了后续所有大型语言模型的设计哲学。

自然语言处理的道路依然漫长，让机器真正理解人类语言的奥秘，或许是人类赋予人工智能的最艰巨也最迷人的挑战之一。但毫无疑问，我们已经踏上了一条由数据、算法和算力共同铺就的快速道路，正在不断逼近那个看似遥不可及的终极目标。"""
]

# 文本预处理
def preprocess(text):
    text = re.sub(r'[^\w\s]', '', text)
    words = jieba.lcut(text)
    stopwords = {'的', '是', '在', '中', '由', '为', '可以', '将', '需要'}
    return [w for w in words if w not in stopwords and len(w) > 1]

# 处理语料并训练模型
sentences = [preprocess(text) for text in corpus]
model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, sg=1)  # Skip-gram模型

# 2. 游戏核心逻辑
def word_guess_game():
    # 从词汇表中随机选择目标词
    target_words = list(model.wv.key_to_index.keys())
    target = random.choice(target_words)
    
    # 获取最相似的3个词（排除自身）
    try:
        similar_words = [w for w, _ in model.wv.most_similar(target, topn=5) if w != target][:3]
    except:
        print("游戏初始化失败，换一个目标词重试")
        return
    
    # 游戏交互
    print("\n🎮 词义猜谜游戏：根据以下相似词，猜出目标词（中文）")
    print(f"提示：与这些词最相似 -> {similar_words}")
    
    # 学生输入答案（可在Colab中直接输入）
    guess = input("请输入你的猜测：")
    
    # 验证答案
    if guess == target:
        # 计算相似度（展示模型分数）
        similarity = model.wv.similarity(guess, similar_words[0])
        print(f"🎉 猜对了！目标词就是「{target}」")
        print(f"相似度参考：与「{similar_words[0]}」的相似度为 {similarity:.2f}")
    else:
        print(f"❌ 猜错啦，正确答案是「{target}」")

# 启动游戏
if __name__ == "__main__":
    word_guess_game()
    # 可多次运行：print("\n再来一局！")；word_guess_game()
